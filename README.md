# simulacra
simulacra is a framework for creating and testing agents simulating intelligent, interactive behavior using large language models.

Large Language models (LLMs) act as stateless natural language computation engines, where their world view is generated in an instant based on their training data and the context that is provided in each prompt. The key to harnessing their power for creating systems that provide immersive experiences useful for all types of interactive media is in creating the supporting frameworks that structure prompts to provide everything necessary for the model to understand and successfully synthesise an appropriate output that gives the allusion of intelligence and continuity of thought. 

To that end, the LLM must be supplied in every call with contextual queues, a memory of past interactions, and effective guidance in the processing of each query for it to successfully produce the desired results.

## Features

- Automatically generate agents with random personality traits, motivations and backstory
- Autmatically create a world and specific location that the character inhabits
- Allow the agents to freely go about their lives, interact with each other, develop friendships, adversaries, or anything else that normally happens in the life of an individual
- Talk to any agent at any point in their existence and see how their story and personality are evolving
- Query an agents memories, 

## Commands

After building and running, an environment (stored in memory as an environment tree) will have been created. You can then add people to the environment and they will automatically go about their lives. A running history of activities and interactions will be displayed as they occur. At any time you can see what is happening with your people, or modify some aspect of the simulation using available commands. 

## Installation

To run this project, you'll need Rust installed on your machine. You can follow the instructions at https://www.rust-lang.org/tools/install.

For a data storage, this project uses [RocksDB](https://rocksdb.org/), a high performance, embeddable persistent key-value store. This requires [LLVM compiler](https://github.com/llvm/llvm-project) be installed on your system. You can find the latest installer for LLVM appropriate for your os [here](https://github.com/llvm/llvm-project/releases/). 

## Clone the repository
```
git clone https://github.com/dmisino/simulacra.git
cd asimulacra
```

### Configure API keys

This project currently uses OpenAI's GPT-3.5-Turbo language model. I will add options to use a locally running language model such as LLaMa (which you can grab from my [docker account](https://hub.docker.com/r/dmisino/dalai)), but for now you'll need an OpenAI API account. To set environment variables use the following:
```
# Windows
set OPENAI_API_KEY="your_openai_api_key"
# Linux
export OPENAI_API_KEY="your_openai_api_key"
```

### Build and run

```
cargo run --release -p simulacra_cli
```

## Usage

TBD

## How to create believable agents

### Creating a world, a location and an agent

TBD

### Letting agents live thier lives

TBD

### Saving memories

Agents perform whatever actions or interactions are a part of their programming. This is specific to each implementation. Given some interaction or event, the text of this event (presumably generated by a language model) is fed to a languge model which is tasked with extracting a concise list of memories from the overall text. These are de-duped and saved as memories.

### Retrieving memories

When prompting a language model for an agent action at each time interval, the memories most relevant to the current situation need to be included in the prompt. Surfacing relevant memories is a significant challenge. One method invloves ranking memories based on:
  
  1. Recency - An exponential decay function is used to reduce the relevance of older memories.
  2. Relevance - A language model prompt describes the current situation, and asks for a ranking from 1 to 10 of how relevant each memory is to the current situation. 
  3. Importance - This refers to how important the memory is to the individual agent. A language model prompt describes the agent, its purpose, personality, memories and so on as is available, and then asks for a ranking from 1 to 10 of how important each memory is likely to be to that individual.

  The results of these three rankings are combined to produce a final ranking for each memory. The top memories that fit within the prompt context are then passed to the language model to be included in the  prompt to generate an agent action or response.

### Reflections

Reflections are new thoughts formed from existing memories. On occasion the system will pull available memories, and send them to a language model with instructions to "reflect" on those memories and draw new thoughts or conclusions, which are then saved as new memories so they can help influence future behaviors. This allows for more organic and natural behavior from agents.

## Acknowledgements

[rozgo](https://github.com/rozgo)

[Generative Agents: Interactive Simulacra of Human Behavior](https://arxiv.org/pdf/2304.03442.pdf)

[ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/pdf/2210.03629.pdf)
